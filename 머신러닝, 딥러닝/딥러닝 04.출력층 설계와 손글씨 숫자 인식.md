# 3.5 출력층 설계하기

## 3.5.0 도입

기계학습 문제는 분류와 회귀로 나뉩니다. 일반적으로 회귀에는 항등함수, 분류에서는 소프트 맥스 함수를 사용한다.

> 분류: 데이터가 어느 클래스에 속하는가에 대한 문제

- 사진속 인물의 성별을 분류하는 문제
- **이산확률변수**는 확률변수 X가 취할 수 있는 값이 유한하기 때문에 셀 수 있는 확률변수다.  '한 개의 동전을 두 번 던질 때 앞면이 나오는 횟수'와 '한 개의 주사위를 두 번 던질 대 눈금의 합'을 확률변수들로 삼으면, 이 확률변수들은 이산확률변수다. 확률변수가 취할 수 있는 값이 3개, 11개로 유한하기 때문이다.

> 회귀: 입력데이터에 대한 (연속적인) 수치를 예측하는 문제

- 사진 속 인물의 몸무게를 예측
- **연속확률변수**는 확률변수 X가 취할 수 있는 값이 어떤 범위에 속하는 모든 실수로 무한하기 때문에 셀 수 없는 확률 변수다. 귤이 100개 들어있는 상자가 있는데 그 안에 귤의 무게를 확률변수로 삼으면, 이 확률변수는 연속확률변수다. 확률변수가 취할 수 있는 값이 50g 이상 150g 이하에서(예를 들자면) 연속적으로 존재하기 때문이다. 10g, 20g, 30g 이런 식으로 결코 딱딱 안 떨어질 것이다. 아마도 101g, 97g 등 이런 식으로 값이 존재할 것이다.



## 3.5.1 항등 함수와 소프트맥스 함수 구현하기

> 항등함수(회귀)

- 입력을 그대로 출력한다.
- 항등 함수(x = y)

<img src="C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\밑바닥부터 시작하는 딥러닝\img\fig3-21.png" alt="fig3-21" style="zoom: 50%;" />

>  소프트맥스 함수(exp(x) = e^x(지수함수를 나타낸다.))(분류)

<img src="C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\밑바닥부터 시작하는 딥러닝\img\e3.10.png" alt="e3.10" style="zoom:80%;" />

- n: 출력층의 뉴런수
- y_k는 그 중 k번째 출력을 뜻한다.
- 소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받는다. 분모에서 보듯, 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문이다.

<img src="C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\밑바닥부터 시작하는 딥러닝\img\fig3-22.png" alt="fig3-22" style="zoom:50%;" />

```python
import numpy as np
a = np.array([0.3, 2.9, 4.0])
exp_a=np.exp(a)
print(exp_a)

>>>
[ 1.34985881 18.17414537 54.59815003]
```

```python
sum_exp_a = np.sum(exp_a)
y = exp_a / sum_exp_a
print(y)

>>>
[0.01821127 0.24519181 0.73659691]
```

![image-20220321190757247](C:\Users\vjkim\AppData\Roaming\Typora\typora-user-images\image-20220321190757247.png)

```python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```



## 3.5.2 소프트맥스 함수 구현 시 주의점

softmax()의 함수는 위의 식을 제대로 표현하고 있지만 컴퓨터로 계산할 때 결함이 존재한다.

오버플로 문제

- 지수함수를 연산했을 경우 아주 큰 값이 출력된다.
- e^1000은 무한대를 뜻하는 inf가 되어 돌아온다.
- 아주 큰 수를 나눗셈 했을 때 결과가 불안정해 진다.
- 너무 큰 값으로 인한 overflow이기 때문에 큰 값을 작게 만들어주면 해결

```python
np.exp(1000)

>>>
inf
```

- 오버플로우 문제를 해결하기 위해 소프트맥스 함수 구현을 개선해야 함

  ![image-20220321191341895](C:\Users\vjkim\AppData\Roaming\Typora\typora-user-images\image-20220321191341895.png)

<img src="C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\밑바닥부터 시작하는 딥러닝\img\e3.11.png" alt="e3.11" style="zoom:33%;" />

- 이 때, C/'에 어떤 값을 대입해도 상관없지만, 오버플로를 막을 목적으로는 입력 신호 중 최댓값을 입력해주는게 일반적이다.

![image-20220321191526332](C:\Users\vjkim\AppData\Roaming\Typora\typora-user-images\image-20220321191526332.png)

```python
# nan은 not a number
# 아무런 조치 없이 그냥 계산하면 nan이 출력
a = np.array([1010, 1000, 990])
np.exp(a)/np.sum(np.exp(a))

>>>
array([nan, nan, nan])
```

```python
# 입력 신호 중 최댓값을 빼주면 올바르게 계산 가능
a = np.array([1010, 1000, 990])
c = np.max(a)
np.exp(a-c)/np.sum(np.exp(a-c))

>>>
array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])
```

```python
def softmax(a):
    c=np.max(a)
    exp_a=np.exp(a-c) # 오버플로 대책
    sum_exp_a = np.sum(exp_a)
    y = exp_a/sum_exp_a
    return y
```



## 3.5.3 소프트맥스 함수의 특징

```python
a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)
np.sum(y)

>>>
[0.01821127 0.24519181 0.73659691]
1.0
```

- 소프트맥스 함수의 출력은 0에서 1.0 사이의 실수이다. 소프트맥수 함수 출력의 총합은 1이다.
- 이는 소프트맥스 함수의 출력을 '확률'로 해석할 수 있다.
- 위의 결과에서 y[0]은 1.8%, y[1]은 24.5%, y[2]는 73.7%로 해설할 수 있다.
- 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다. 왜냐하면 지수함수는 단조 증가 함수이기 때문이다.
  - 단조 증가: 함수 f가 임의의 a,b에 대해 a<=b이면 f(a)<=f(b)

>기계학습의 문제 풀이는 학습과 추론 두 단계를 거쳐 이루어 진다.
>
>1. 학습 단계에서 모델을 학습
>
>2. 추론 단계에서 앞서 학습한 모델로 미지의 데이터에 대한 추론(분류)를 수행





# 3.6 손글씨 숫자 인식

## 3.6.0 도입

신경망의 실전 예를 적용해보자!

손글씨 숫자 분류

1. 이미 학습된 매개변수를 사용하여 학습의 과정은 생략
2. 추론의 과정만 구현한다.
3. 추론의 과정은 신경망의 순전파라고도 한다.



## 3.6.1 MNIST 데이터셋

- 손글씨 숫자 이미지 집합
- 실험용 데이터로 많이 사용된다.
- 0~9까지의 숫자 이미지로 구성(훈련 60,000장, 시험 10,000장)
- 28 x 28 크기의 회색조 이미지(1채널), 각 픽셀은 0에서 255까지의 값을 취한다.

코드 설명

모듈

- sys 모듈을 이용해 부모 디렉터리의 파일을 가져올 수 있도록 설정
- dataset 디렉터리 안에 있는 mnist.py의 load_mnist 함수를 임포트 한다.
  - 첫 번째는 인터넷에서 다운로드하며 이후부터는 Pickle 파일에 의해 로컬에 저장된 파일을 읽어온다.
  - load_mnist 함수는 읽은 MNIST 데이터를 "(훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블) "형식으로 반환한다.
    인수는 3개 존재(훈련이미지는 실제 숫자의 이미지 파일, 레이블은 이미지의 실제 값)
    - normalize: 입력 이미지의 픽셀값을 0.0 ~ 1.0 사이의 값으로 정규화할지를 정한다.(기존값은 0~255)
    - flatten: 입력 이미지를 평탄값 즉, 1차원 배열로 만들지를 정한다.(false를 입력하면 1x28x28의 3차원 배열, True를 입력하면 1차원 배열 784개의 원소)
    - one_hot_label: True면, 레이블을 원-핫(one-hot) 배열로 돌려준다. one-hot 배열은 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.(기본값은 False)
    - fromarray: 넘파이로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환해야 한다.

> pickle 프로그램: 실행 중에 특정 객체를 파일로 저장하는 기능이다. 저장해둔 pickle파일을 로드하면 실행 당시의 객체를 즉시 복원할 수 있다.

```python
!pip install pillow
```

```python
import sys, os

# 부모 디렉터리(code)의 파일을 가져올 수 있도록 설정
sys.path.append("./code/")

# load_mnist 함수는 읽은 mnist 데이터를 (훈련이미지,훈련레이블), (시험 이미지, 시험 레이블) 형식으로 반환
from dataset.mnist import load_mnist


import numpy as np
from PIL import Image

def img_show(img):
    pil_img=Image.fromarray(np.uint8(img))
#    pil_img.show()
    return pil_img

# nomalize는 입력 이미지의 픽셀값을 0~255값이 아닌 0~1사이의 값으로 정규화할지 정함
# flatten은 입력 이미지를 평탄하게, 즉 1차원 배열로 만들지를 정함
(x_train, t_train),(x_test, t_test)=load_mnist(flatten=True,normalize=False)
t_train

>>>
array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)
```

```python
t_train[3]

>>>
1

x_train[3].shape

>>>
(784,)

x_train[3].reshape((28,28))

>>>
대충 1모양의 배열이 나옴
```

- 이미지를 1차원 넘파이 배열로 저장했기 때문에 이미지를 다시 표시하기 위해선 28x28 크기로 다시 변형해야 한다.
- reshape() 메서드를 통해 넘파이 배열의 형상을 바꿀 수 있다.

```python
img = x_train[3] # 0번째에는 5, 1번째에는 0이 있는걸 확인할수 있다.
label = t_train[3] 
img_1=img.reshape(28,28)
print(img_1.shape)
img_show(img_1)

>>>
(28,28)
숫자1 이미지 출력
```

```
label

>>>
1


Image.fromarray(np.uint8(img_1))
>>>
숫자 1 이미지 출력
```

- x_train : train data set
- t_train : train data set 의 정답값
- x_test : test data set
- t_test : test data set 의 정답값



## 3.6.2 신경망의 추론 처리

MNIST 데이터셋을 가지고 추론을 수행하는 신경망을 구현하자.

- 입력층 뉴런 784개(원소의 갯수), 출력층 뉴런 10개로 구성(0~9까지 판정)
- 은닉층은 총 두 개
- 첫 번째 은닉층은 50개의 뉴런
- 두 번째 은닉층은 100개의 뉴런
- init_network() 에서는 pickle 파일인 sample_weight.pkl에 저장된 '학습된 가중치 매개변수'를 읽는다. 가중치와 편향 매개변수가 딕셔너리 변수로 저장되어 있따.
- 이미 학습된 매개 변수를 이용하기 때문에 test_Set만 불러와서 진행한다.

```python
import pickle

# load_mnist 함수는 읽은 mnist 데이터를 (훈련이미지,훈련레이블), (시험 이미지, 시험 레이블) 형식으로 반환
# nomalize는 입력 이미지의 픽셀값을 0~255값이 아닌 0~1사이의 값으로 정규화할지 정함
# flatten은 입력 이미지를 평탄하게, 즉 1차원 배열로 만들지를 정함
def get_data():
    (x_train, t_train),(x_test, t_test)=load_mnist(flatten=True,normalize=False)
    return x_test,t_test
    
# pickle 파일인 sample_weight.pkl에 저장된 학습된 가중치 매개변수를 읽습니다.    
def init_network():
    with open(".\code\ch03\sample_weight.pkl","rb") as f:
        network = pickle.load(f)
    return network

def sigmoid(x):
    return 1/(1+np.exp(-x))

def softmax(a):
    c=np.max(a)
    exp_a=np.exp(a-c) # 오버플로 대책
    sum_exp_a = np.sum(exp_a)
    y = exp_a/sum_exp_a
    return y

def predict(network,x):
    W1, W2, W3 = network['W1'],network['W2'],network['W3']
    b1, b2, b3 = network['b1'],network['b2'],network['b3']
    
    a1=np.dot(x,W1)+b1
    z1=sigmoid(a1)
    a2=np.dot(z1,W2)+b2
    z2=sigmoid(a2)
    a3=np.dot(z2,W3)+b3
    y=softmax(a3)
    
    return y
```

```python
network = init_network()
type(network)
>>>
dict

network['W1'].shape
>>>
(784, 50)

network['W2'].shape
>>>
(50, 100)

network['W3'].shape
>>>
(100, 10)

network['b1'].shape
>>>
(50,)

network['b2'].shape
>>>
(100,)

network['b3'].shape
>>>
(10,)
```

> 평가

신경망에 의한 추론을 수행해보고, 정확도(분류가 얼마나 올바른가)를 평가해 보자.

- 가장 먼저 MNIST 데이터 셋을 얻고 네트워크를 생성
- for문을 돌려 x에 저장된 이미지 데이터를 1장씩 꺼내 predict() 함수로 분류
- predict() 함수는 각 레이블의 확률을 넘파이 배열로 반환
- 예를 들어 [0.1, 0.3, ...]이라하면 이미지가 숫자 0일 확률은 10%라고 해석할 수 있다.
- 그런 다음 np.argmax()함수를 이용해 배열에서 값이 가장 큰 원소의 인덱스를 구한다.

```
```

