# 4.2 손실 함수

## 4.2.0 도입

손실함수

손실함수란 신경망 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄

- 비용함수(Cost Function)이라고도 불린다.
- 손실에는 그만큼의 비용이 발생하기 때문

예를들어 총 10개의 값으로 0부터 9까지의 숫자 중 어떤 값이진 나타내는 확률이 나열된 배열을 생각해보자.

```python
y=[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t=[0,0,1,0,0,0,0,0,0,0]
```

- y는 소프트맥스 함수의 출력이다.
- t는 원-핫 인코딩 방식을 이용한 표기법이다.
- 실제로 위와 같은 결과물에 대한 손실을 구해봅시다.



## 4.2.1 평균 제곱 오차

가장 많이 쓰이는 손실함수는 **평균 제곱 오차(MSE)**이다.
<img src="C:\Users\vjkim\Python\밑바닥부터 시작하는 딥러닝\img\e4.1.png" alt="e4.1" style="zoom:50%;" />

- y는 신경망의 출력(신경망이 추정한 값)
- t는 정답 레이블
- k는 데이터의 차원 수
- 1/2는 크게 의미없는 수자 같지만 추후 미분을 위해 존재함

```python
import numpy as np
def mean_squared_error(y, t):
    return 0.5*np.sum((y-t)**2)
# for문을 이용해서 구현해야 할 것 같지만 for문을 사용할 필요가 없다. 배열의 장점!
```

```python
y=[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t=[0,0,1,0,0,0,0,0,0,0]

mean_squared_error(np.array(y),np.array(t))

>>>
0.09750000000000003
```



## 4.2.2 교차 엔트로피 오차

또 다른 손실함수로 **교차 엔트로피 오차**를 사용한다.
<img src="C:\Users\vjkim\Python\밑바닥부터 시작하는 딥러닝\img\e4.2.png" alt="e4.2" style="zoom:50%;" />

- log는 밑이 e인 자연로그(ln)이다.
- y는 신경망 출력
- t는 정답 레이블(정답에 해당하는 인덱스의 원소만 1이고 나머지는 0인 원-핫 인코딩 사용)
- t = 1(즉, 정답)일 때만 자연 로그를 계산하는 식이다.
- 맨 앞이 음수처리된 이유는 신경망의 출력값이 0~1 사이의 값이기 때문에 양수로 만들어주기 위해서이다.
- 예시)
  - 정답 레이블 t = '2'
  - 신경망 출력 y = '0.6'
  - 교차 엔트로피 오차는 -log0.6 = 0.51
  - 같은 조건에서 신경망 출력이 '0.1'이라면 -log0.1 = 2.30이 된다.
  - 신경망의 결과값이 정답에 가깝게 나타날 경우 (1에 가까운 숫자) 오차 값이 줄어들고, 0에 가깝게 나타날 수록 오차는 커진다.

```python
import matplotlib.pyplot as plt
x = np.arange(0.01, 3, 0.005)
f = np.log(x)
plt.plot(x, f)
plt.show()
```

![다운로드](C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\다운로드.png)

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t*np, log(y + delta))
```

- e = 10을 의미한다.
- 1e - 7 = 1/10^7
- delta를 더해준 이유는 np.log() 함수에 0을 입력하여 마이너스 무한대가 되기 때문에 더 이상 계산을 진행할 수 없다. 
  따라서 0이 되지 않기 위함

