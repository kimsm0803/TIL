# 4.5 학습 알고리즘 구현하기

## 4.5.0 도입

> 전체

신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.

1단계 - 미니배치

훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표

2단계 - 기울기 산출

미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.

3단계 - 매개변수 갱신

가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.

4단계 - 반복

1~3단계를 반복하여 수행합니다.

- 데이터를 미니배치로 무작위로 선장하기 때문에 확률적 경사 하강법이라고 부른다.(확률적으로 무작위로 골라낸 데이터에서 수행하는 경사 하강법)



## 4.5.1 2층 신경망 클래스 구현

- 2층 신경망을 하나의 클래스로 구현해 보자.

필요한 것들

1. 매개변수 설정
2. 미니 배치
3. 예측값 출력
4. 손실함수 생성
5. 손실함수 기울기 산출
6. 매개변수 갱신

입력값에 대한 shape를 미리 알고 있어야 한다.

```python
import sys, os
sys.path.append("./code/") # 부모 디렉터리의 파일을 가져올 수 있도록 설정
from common.functions import *
from common.gradient import numerical_gradient
import numpy as np

class TwoLayerNet:
    
    # 초기화를 수행한다. 인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        # params 신경망의 매개변수를 보관하는 딕셔너리 변수
        self.params = {}
        self.params['W1'] = weight_init_std * np.randon.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    # 예측(추론)을 수행한다. 인수 x는 이미지 데이터
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
    
    # x: 입력(이미지) 데이터, t: 정답 레이블
    # 손실 함수의 값을 구한다.
    def loss(self, x, t):
        y = self.perdict(x)
        
        return corss_entropy_error(y, t)
    
    # 정확도를 구한다.
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    
    # x: 입력(이미지) 데이터, t: 정답 레이블
    # 가중치 매개변수의 기울기를 구한다.
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        # grads: 기울기 보관하는 딕셔너리 변수(numeriacl_gradient() 메서드의 반환 값)
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
    
    # 가중치 매개변수의 기울기를 구한다.numerical_gradient()의 성능 개선판
    def gradient(self, x, t):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]
        
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        # backward
        dy = (y - t) / batch_num
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis =0)
        
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)
        
        return grads
```



**np.random.randn(input_size, hidden_size)**

- 가중치 매개변수를 추출할 때 사용한 함수이다. 클래스의 초기화 함수에 존재함
- 가중치 매개변수의 초깃값을 무엇으로 설정하느냐가 신경망 학습의 성공을 좌우한다.
- 당장은 정규분포를 따르는 난수로 편향은 0으로 초기화한다.

```python
net= TwoLayerNet(784, 100, 10)
net.params['W1'].shape

>>>
(784, 100)
```

```python
net.params['b1'].shape

>>>
(100, )

net.params['W2'].shape

>>>
(100, 10)
```

- params 변수에는 이 신경망에 필요한 매개변수가 모두 저장된다.
- params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용된다. 참고로 예측 처리는 다음과 같이 실행할 수 있다.

```python
x = np.random.rand(100,784) # 0부터 1사이의 균일 분포에서 난수 생성 더미 입력 데이터로 테스트
y = net.predict(x)
```



## 4.5.2 미니배치 학습 구현

- 미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고 미니배치
- 미니배치에 대해서 경사법으로 대개변수를 갱신한다.
- TwoLayerNet 클래스와 MNIST 데이터셋을 사용하여 학습을 수행해보자.

```python
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist

# 데이터 읽기
(x_train, t_train), (x_text, t_text) = load_mnist(nomalize=True, one_hot_label = True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 하이퍼파라미터

```

