

# 3.1 신경망

## 3.1.0도입

퍼셉트론은 장점과 단점이 함께 존재 한다.

- 장점
  - 퍼셉트론으로 복잡한 함수도 표현할 수 있다.
  - 그 예로 컴퓨터가 수향되는 복잡한 처리도 퍼셉트론으로 표현할 수 있다.(이론상으로)
- 단점
  - 가중치를 설정하는 작업은 사람이 수동으로 조작해 줘야 한다.

## 3.1.1 신경망의 예

신경망을 그림으로 나타내면 다음과 같이 이루어져 있다.

- 은닉층의 뉴런은 사람 눈에는 보이지 않는다.(그래서 '은닉')
- 아래의 신경망은 모두 3층으로 구성 되었지만, 가중치를 갖는 층은 2개 뿐이므로 '2층 신경망'이라고도 한다.

<img src="C:\Users\vjkim\AppData\Roaming\Typora\typora-user-images\image-20220318164423608.png" alt="image-20220318164423608" style="zoom:50%;" />

## 3.1.2 퍼셉트론 복습

<img src="C:\Users\vjkim\AppData\Roaming\Typora\typora-user-images\image-20220318164550720.png" alt="image-20220318164550720" style="zoom:50%;" />

- 위의 그림은 x1과 x2라는 두 신호를 입력받아 y를 출력하는 퍼셉트론이다. 이를 수직으로 변경하면 다음과 같이 표현 가능하다.

![image-20220318164712867](C:\Users\vjkim\AppData\Roaming\Typora\typora-user-images\image-20220318164712867.png)

- b: 뉴런이 얼마나 쉽게 활성화 되느냐를 제어하는 편향
- w1, w2: 각 신호의 영향력을 제어하는 가중치 매개변수
- 편향을 표기하여 퍼셉트론을 표기하면 다음과 같이 나타낼 수 있다.

<img src="C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\밑바닥부터 시작하는 딥러닝\img\fig3-3.png" alt="fig3-3" style="zoom:50%;" />

- 가중치가 b이고 입력이 1인 뉴런
- 위 퍼셉트론의 동작은 x1, x2, 1이라는 3개의 신호가 뉴런에 입력되어, 각 신호에 가중치를 곱한 후, 다음 뉴런에 전달된다.
- 다음 뉴런들에서는 이 신호의 값을 더하여, 그 합이 0을 넘으면 1을 출력, 그렇지 않으면 0을 출력한다.

<img src="C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\밑바닥부터 시작하는 딥러닝\img\e3.2.png" alt="e3.2" style="zoom:50%;" />

<img src="C:\Users\vjkim\OneDrive\바탕 화면\강의\머신러닝\밑바닥부터 시작하는 딥러닝\img\e3.3.png" alt="e3.3" style="zoom:50%;" />

- 입력 신호의 총합이 h(x)라는 함수를 거쳐 변환되어, 그 변환된 값이 y로 출력됨을 보여준다.



## 3.1.3 활성화 함수의 등장

방금 h(x)라는 함수가 등장했는데 이를 활성화 함수라고 한다.

>  활성화 함수

- 입력 신호의 총합을 출력신호로 변환하는 함수
- '활성화'라는 이름이 말해주듯 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다.
- 계산 과정
  1. 가중치가 달린 입력 신호와 편향의 총합을 계산, 이를 a라고 한다.
  2. a를 h() 함수에 넣어 y라는 값을 출력한다.

<img src="C:\Users\vjkim\AppData\Roaming\Typora\typora-user-images\image-20220318171553242.png" alt="image-20220318171553242" style="zoom:50%;" />

- 가중치 신호를 조합한 결과가 a라는 노드가 되고, 활성화 함수 h()를 통과하여 y라는 노드로 변환되는 과정이 분명하게 나타나있다.(노드와 뉴런을 같은 의미로 보겠다.)
- 그림에서 왼쪽은 일반적인 뉴런, 오른쪽은 활성화 처리 과정을 명시한 뉴련 (a는 입력신호의 총 합, h()는 활성화 함수 y를 출력)



# 3.2 활성화 함수

## 3.2.0 도입

활성화 함수는 임계값을 경계로 출력이 바뀌는 데, 이런 함수를 계단함수라 한다.

- 퍼셉트론에서 활성화 함수로 계단함수를 이용한다.
- 계단함수 이외의 함수를 퍼셉트론에 사용하면 어떻게 될까?



## 3.2.1 시그모이드 함수

![img](3.1 신경망.assets/e3.6.png)

- exp(-x) = e^(x)
- e는 자연상수로 2.7182... 값을 갖는 실수
- 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환, 그변환된 신호를 다음 뉴런에게 전달한다.



## 3.2.2 계단 함수 구현

![img](3.1 신경망.assets/e3.3-16475957778801-16475957782302.png)

- 입력이 0을 넘기면 1을 출력
- 이외에는 0을 출력

```python
def step_function(x):
    if x > 0:
        return 1
    else:
        return 0
```

- 위와 같은 함수로 구현한 결과의 문제점은 인수 x를 실수만 받아들인다는 것이다.
- 실수인수를 넣을 수는 있지만 넘파이 배열을 인수로 넣을 수는 없다. 예) step_function(np.array([1.0, 2.0]))과 같이 사용 불가



**넘파이 배열도 사용할 수 있도록 구현해보기**

1. numpy 배열에 부등호 연산을 수행하면 각각 부등호 연산을 수행한 bool 배열이 생성된다.
2. 그 값을 별도로 저장한다.
3. bool type을 int형으로 변경한다.
   - 넘파이 자료형의 데이터를 변환할 때는 astype() 매서드를 이용한다.
   - 원하는 자료형을 인수로 지정하면 된다. (예) np.int, np.str 등

```python
import numpy as np
x_1=np.array([-1.0,1.0,-2.0,0.5])
y_1=x_1 > 0
print(y_1)
print(type(y_1))
print(y_1.astype(np.int))
type(y_1.astype(np.int)[0])

>>>
[False  True False  True]
<class 'numpy.ndarray'>
[0 1 0 1]
numpy.int32
```

```
import numpy as np
def step_function_1(x):
    y = x >0
    return y.astype(np.int)
    
x_2=np.array([-1.0,1.0,-2.0])
y_2=step_function_1(x_2)
```

## 3.2.3 계단 함수의 그래프

```python
import numpy as np
import matplotlib.pylab as plt
x_3 = np.arange(-5.0, 0.5, 0.1) # -5에서 5 전까지 0.1 간격으로 넘파이 배열을 생성
y_3 = step_function_1(x_3)
plt.plot(x_3,y_3)
plt.ylim(-0.1,1,1)
plt.show()
```

<img src="3.1 신경망.assets/fig3-6.png" alt="img" style="zoom:33%;" />



## 3.2.4 시그모이드 함수 구현

```python
def sigmoid(x):
    return 1/1(+np.exp(-x))
```

```python
x_4 = x_3
y_4 = sigmoid(x_4)
plt.plot(x_4,y_4)
plpt.ylim(-0.1,1.1)
plt.show()
```

<img src="3.1 신경망.assets/fig3-7.png" alt="img" style="zoom: 33%;" />



## 3.2.5 시그모이드 함수와 계단 함수 비교

<img src="3.1 신경망.assets/fig3-8.png" alt="fig3-8" style="zoom:33%;" />

- 가장 직관적인 차이는 매끄러움의 차이
  - 시그모이드 함수는 부드러운 곡선이며, 입력에 따라 출력이 연속적으로 변경된다.
  - 계단 함수는 0을 경계로 출력이 갑자기 바뀌어 버린다.
- 시그모이드 함수의 매끈함이 신경망 학습에 중요한 역할을 차지하고 있다.
  - 퍼센트론(계단함수): 0혹은1이 흐른다.
  - 신경망(시그모이드함수): 연속적인 실수
- 하지만 둘 다 입력이 아무리 작거나 커도 출력은 0에서 1사이
- 둘 다 선형함수이다.



## 3.2.6 비선형 함수

시그모이드 함수는 곡선, 계단 함수는 계단처럼 구부러진 직선으로 나타내며, 동시에 비선형 함수로 분류된다.

> 선형 함수는 무언가 입력했을 때 출력이 입력의 상수배 만큼 변하는 함수이다. f(x) = ax+b 인 1개의 곧은 직선이 되는 함수이다.

신경에서는 활성화 함수로 비선형함수를 사용해야 한다. 그 이유는 선형함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문이다.

- 선형함수의 문제는 층을 아무리 깊게해도 '은닝층이 없는 네트워크'로도 똑같은 기능을 할 수 있다.
  - h(x) = cs 를 활성화 함수로 사용한 3층 네트워크를 떠올려 봅시다.
  - 결과는 y(x) = h(h(h(x))) = c*c*c*x일 뿐입니다.
  - 이는 y(x) = c^3*x 와 다를 게 없는 결과이므로 은닉층이 없는 네트워크로 표현할 수 있습니다.



## 3.2.7 ReLU 함수

시그모이드 함수를 이용해 신경망을 구성했지만 최근에는 ReLU 함수를 주로 이용합니다.

- ReLU 함수는 입력이 0을 넘으면 그 입력을 그대로 출력합니다.
- 0 이하면 0을 출력하는 함수입니다.

```python
def relu(x):
    return np.maximum(0,x)
```

```python
x_7 = x_3
y_7 = relu(x_7)
plt.plot(x_7,y_7)
plt.show()
```

<img src="3.1 신경망.assets/fig3-9.png" alt="fig3-9" style="zoom:33%;" />
